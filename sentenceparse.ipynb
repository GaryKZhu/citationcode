{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25123436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2fc4ea6354c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m#find all occurences of {{cite: }}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mfindCitations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m#add the number of sentences to the total number of sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2fc4ea6354c3>\u001b[0m in \u001b[0;36mfindCitations\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m#add sentence to db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mcreate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marxiv_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcitations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtotcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#append sentence into db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m#clear for next sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2fc4ea6354c3>\u001b[0m in \u001b[0;36mcreate_sentence\u001b[0;34m(conn, sentence)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sqlite3\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "#declare global variables\n",
    "fixedSentences = []\n",
    "citations = []\n",
    "uidarr = []\n",
    "sentences = []\n",
    "totcount = 0 \n",
    "filename = \"\"\n",
    "arxiv_id = \"\"\n",
    "def readfile(filename):\n",
    "    \n",
    "    #open a file\n",
    "    f = open(filename, 'rt')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def parsefile(text):\n",
    "    #use string tokenize\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def fixSentences():\n",
    "    \n",
    "    #fix sentences. There is a bug in unarXive dataset where two sentences are merged into one. \n",
    "    #This happens when a sentence has a Formula, and then the next word starts with a capital letter. \n",
    "    \n",
    "    for s in sentences:\n",
    "        \n",
    "        #remove bad formatting\n",
    "        #remove newlines\n",
    "        s = re.sub(r\"\\r|\\n\", \" \", s)\n",
    "        \n",
    "        #remove double space\n",
    "        s = re.sub(r\"\\s\\s+\", \" \", s)\n",
    "        \n",
    "        #remove spacing of punctuation.  Ex: Turning \"  .\"into \".\", \"  ,\" into \",\" etc. \n",
    "        s = re.sub(r\" *\\.\", \".\", s)\n",
    "        s = re.sub(r\" *,\", \",\", s)\n",
    "        s = re.sub(r\" *\\).\", \")\", s)\n",
    "        \n",
    "        #prepare second string variable\n",
    "        s1 = \"\"\n",
    "\n",
    "        #find all occurences of \"FORMULA\", init variable to tell if we divided or not. \n",
    "        indices = [m.start() for m in re.finditer(\"FORMULA\", s)]\n",
    "        divide = False\n",
    "\n",
    "        #check if the next word after FORMULA has a capital letter in the first one. \n",
    "        #this can result in a bug, where it is something like FORMULA CDMP. Will improve soon to check the entire word. \n",
    "\n",
    "        \n",
    "        #list of bugs discovered that can cause noise.\n",
    "        # REF -> Splits a sentence into 2.\n",
    "        # FORMULA and FIGURE also can split a sentence into 2.\n",
    "        # i.e. and e.g. cause a different bug where it splits 1 into 2.\n",
    "        \n",
    "        for i in indices:\n",
    "            if(i+8 < len(s) and s[i+8].isupper() and i+8 not in indices):\n",
    "                s1 = s[i+8:]\n",
    "                s = s[:i+8]\n",
    "                if(s[len(s)-1] == \" \"):\n",
    "                    s = s[:len(s)-1]\n",
    "                s += \".\"\n",
    "\n",
    "        #append finish result into array\n",
    "        if(divide == False): \n",
    "            fixedSentences.append(s)\n",
    "        else: \n",
    "            fixedSentences.append(s)\n",
    "            fixedSentences.append(s1)\n",
    "\n",
    "def findCitations(conn):\n",
    "    \n",
    "    #this is to find the the {{cite:  }}\n",
    "    \n",
    "    for i in range(0, len(fixedSentences)):\n",
    "        s = fixedSentences[i]\n",
    "        \n",
    "        #find all indexes of {{cite: and }}. These two arrays store the indexes of these substrings in s. \n",
    "        cinarr = [m.start() for m in re.finditer(\"{{cite:\", s)]\n",
    "        cendarr = [m.start() for m in re.finditer(\"}}\", s)]\n",
    "        \n",
    "        for j in range(0, len(cinarr)): #  parse citations\n",
    "            #get the indexes\n",
    "            citein = cinarr[j]  \n",
    "            citeend = cendarr[j]\n",
    "            \n",
    "            uid = s[citein+7:citeend]  # get the UID using string splice\n",
    "            uidarr.append(uid) # append uid into temporaryarray\n",
    "        \n",
    "        #replace all of the cites \n",
    "        #The Citations will be replaced in order of first occurence, so the first citation will correspond to the first CITE_MARK.\n",
    "        #Relatively easy to search for all occurences of CITE_MARK and then match them. Probably can write another program to do that\n",
    "        \n",
    "        #print(s)\n",
    "        \n",
    "        s = re.sub(r\"{{cite:.*?}}\", \"CITE_MARK\", s) \n",
    "        #fixedSentences[i] = s\n",
    "        \n",
    "        citations.append(len(uidarr)) # update amount of citations for this sentence\n",
    "        \n",
    "        for uid in uidarr: #append all citations into db\n",
    "            add_sentence_citation(conn, (arxiv_id, i+totcount, uid))\n",
    "            \n",
    "        #add sentence to db\n",
    "        create_sentence(conn, (arxiv_id, s, citations[i], i+totcount)) #append sentence into db\n",
    "        \n",
    "        #clear for next sentence\n",
    "        uidarr.clear()\n",
    "\n",
    "\n",
    "def create_sentence(conn, sentence):\n",
    "    \n",
    "    #function to insert all sentences into sqlite\n",
    "    \n",
    "    sql = ''' INSERT INTO sentences (arxiv_id,sentence,citation_count, sentence_id) \n",
    "        VALUES (?, ?, ?, ?) '''\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, sentence)\n",
    "    conn.commit()\n",
    "    return\n",
    "\n",
    "\n",
    "def add_sentence_citation(conn, citation):\n",
    "    \n",
    "    #function to link all citations to their sentences and insert into sqlite\n",
    "    \n",
    "    sql = ''' INSERT INTO citations (arxiv_id, sentence_id,paper_id) \n",
    "        VALUES (?, ?, ?) '''\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, citation)\n",
    "    conn.commit()\n",
    "    return\n",
    "\n",
    "if(__name__ == \"__main__\"):\n",
    "    \n",
    "    #read input\n",
    "    directory = \"/Users/brian/projects/rnn/thesis-master/unarXive-2020/papers\"\n",
    "    filecounter = 0\n",
    "    sconn =  sqlite3.connect(\"sentence.db\")\n",
    "    cconn = sqlite3.connect(\"citation.db\")\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        #get filename\n",
    "        fname  = directory+\"/\"+filename\n",
    "        arxiv_id = filename[:len(filename)-4]\n",
    "        #get input\n",
    "        text = readfile(fname)\n",
    "        sentences = parsefile(text)\n",
    "\n",
    "        #fix formatting of the sentences\n",
    "        fixSentences()\n",
    "\n",
    "        #find all occurences of {{cite: }}\n",
    "        findCitations(sconn)\n",
    "\n",
    "        #add the number of sentences to the total number of sentences\n",
    "        totcount += len(fixedSentences)\n",
    "        fixedSentences.clear()\n",
    "        citations.clear()\n",
    "        filecounter += 1\n",
    "        print(filecounter)\n",
    "        \n",
    "    sconn.close()\n",
    "    cconn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a34ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e52c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
